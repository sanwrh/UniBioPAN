{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Environment and Working Directory Configuration"
      ],
      "metadata": {
        "id": "pdK70SfSw6R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.10.0\n",
        "!pip install cudnn==8.4.1\n",
        "!pip install cudatoolkit==11.8.0\n",
        "!pip install pillow\n",
        "!pip install scikit-learn\n",
        "!pip install openpyxl\n",
        "!pip install opencv\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "cgonus2HlQk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRyIVWSqj7a2",
        "outputId": "320a2e81-1041-48e8-8629-527e44286930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.10.0\n",
            "['/usr/local/lib/python3.10/dist-packages/keras/api/_v2', '/usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/api/_v2', '/usr/local/lib/python3.10/dist-packages/tensorboard/summary/_tf', '/usr/local/lib/python3.10/dist-packages/tensorflow', '/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "print(tf.__version__)\n",
        "tf.__path__\n",
        "print(tf.__path__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lhvTV0J2nrTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Change to the target directory\n",
        "os.chdir('/content/drive/MyDrive/rdkit')\n",
        "# Print the current working directory\n",
        "print(\"The current working directory：\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXlL44tnoJtf",
        "outputId": "3127ee32-24a3-49c9-d11c-5029f1ae5808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前工作目录： /content/drive/MyDrive/rdkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define parameters and file paths."
      ],
      "metadata": {
        "id": "g3ldG0vSzTYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focal Loss Parameters\n",
        "gamma_values = [1]   # Typically between 1.0 and 5.0, increase the focus on samples that are more difficult to classify.\n",
        "pos_weight_values = [1]  # Values less than 1.0 emphasize positive samples in imbalanced datasets where positive samples are rare.\n",
        "# BiLSTM Layer Parameters\n",
        "lstm_units1_values = [32] # Define the number of units for each LSTM layer. (e.g. [32,64,96])\n",
        "num_lstm_layers_values = [1] # Define the number of units for each dense layer. (e.g. [1,2,3])\n",
        "# Dense Layer Parameters\n",
        "dense_units1_values = [32]  # Define the number of units for each dense layer. (e.g. [32,64,96])\n",
        "dense_layers_values = [3] # Define the number of dense layers in the network. (e.g. [1,2,3])\n",
        "# Dropout Layer Parameters\n",
        "dropout_rate1_values = [0.3] # Define the dropout rate for each dropout layer. (e.g. [0.1,0.2,0.3])\n",
        "dropout_layers_values = [1] # Define the number of dropout layers in the network. (e.g. [1,2,3])\n",
        "\n",
        "# Set the number of random iterations\n",
        "num_iterations = 10  # Define the number of iterations for random hyperparameter search.\n",
        "# training set and independent test set path setting\n",
        "train_file = 'Dataset/24_IL-6/IL6_train.xlsx'\n",
        "val_file = 'Dataset/24_IL-6/IL6_test.xlsx'\n",
        "\n",
        "output_path = 'results/results.xlsx'  # The path where the results of training will be saved."
      ],
      "metadata": {
        "id": "YT8aMrUSyD7J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This code block focuses on importing essential modules and defining a custom function for model training. The implemented functionalities within this cell do not necessitate any modifications."
      ],
      "metadata": {
        "id": "UMcff4l1wKqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n",
        "from preprocess_data_test import preprocess_seq, get_max_length, preprocess_sequence\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, balanced_accuracy_score, roc_curve, \\\n",
        "    matthews_corrcoef, precision_score, recall_score, f1_score\n",
        "import math\n",
        "from statistics import stdev\n",
        "import random\n",
        "import logging\n",
        "from focal_loss import BinaryFocalLoss\n",
        "\n",
        "def map_fn(sequence, label):\n",
        "    processed_sequence = tf.py_function(preprocess_sequence, [sequence], tf.float16)\n",
        "    return processed_sequence, label\n",
        "# load_and_preprocess_data\n",
        "def load_and_preprocess_data(sequences, labels):\n",
        "    sequences = tf.constant(sequences, dtype=tf.string)\n",
        "    labels = tf.constant(labels, dtype=tf.int8)\n",
        "\n",
        "    # creat tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
        "    dataset = dataset.map(lambda sequence, label: map_fn(sequence, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(16)\n",
        "    return dataset\n",
        "\n",
        "class PrintMetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch + 1}:\")\n",
        "        print(f\"  loss: {logs['loss']:.4f}\")\n",
        "        print(f\"  accuracy: {logs['accuracy']:.4f}\")\n",
        "        print(f\"  val_loss: {logs['val_loss']:.4f}\")\n",
        "        print(f\"  val_accuracy: {logs['val_accuracy']:.4f}\")\n",
        "        print(f\"  learning_rate: {tf.keras.backend.get_value(model.optimizer.lr):.6f}\")\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.context_vector = self.add_weight(shape=(input_shape[-1], 1), initializer='glorot_uniform',\n",
        "                                              trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute attention scores\n",
        "        attention_scores = tf.matmul(inputs, self.context_vector)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # Apply attention weights to inputs\n",
        "        weighted_inputs = tf.multiply(inputs, tf.expand_dims(attention_weights, axis=-1))\n",
        "        context_vector = tf.reduce_sum(weighted_inputs, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Create a callback function to print relevant information at the end of each epoch\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1} - Train Loss: {logs['loss']:.4f}, Train Accuracy: {logs['accuracy']:.4f}, Test Loss: {logs['val_loss']:.4f}, Test Accuracy: {logs['val_accuracy']:.4f}\")\n",
        "\n",
        "# Define a learning rate callback function to print the learning rate at the end of each epoch\n",
        "class LearningRateCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer.lr.numpy()\n",
        "        print(f\"Epoch {epoch + 1} - Learning Rate: {lr:.6f}\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.1\n",
        "    drop = 0.6\n",
        "    epochs_drop = 3.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
        "    return lrate\n",
        "    # Define the training process\n",
        "def train_model(train_dataset, test_dataset):\n",
        "    strategy = tf.distribute.MirroredStrategy(devices=['GPU:0', 'GPU:1'])\n",
        "    with strategy.scope():\n",
        "        # Define ModelCheckpoint callback\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True,\n",
        "                                                        mode='max', verbose=0)\n",
        "        # EarlyStopping\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
        "\n",
        "        learning_rate = 0.0001\n",
        "        model = create_model()\n",
        "        model.optimizer.lr.assign(learning_rate)\n",
        "        print_learning_rate = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_begin=lambda epoch, logs: print(\n",
        "                f\"Learning Rate: {tf.keras.backend.get_value(model.optimizer.lr):.6f}\"))\n",
        "        callbacks = [reduce_lr, early_stopping, print_learning_rate,checkpoint]\n",
        "        model.fit(train_dataset, epochs=400, batch_size=16, validation_data=test_dataset,\n",
        "                  callbacks=callbacks, verbose=2)\n",
        "    return model\n",
        "\n",
        "# Define model\n",
        "def create_model():\n",
        "    learning_rate = 0.0001\n",
        "    momentum = 0.5\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "\n",
        "    sequence_length = max_length\n",
        "    input_shape = (sequence_length, 32, 32, 3)\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    initializer = tf.keras.initializers.HeNormal(seed=123456)\n",
        "    model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))(\n",
        "        inputs)\n",
        "    for _ in range(2):\n",
        "        model = tf.keras.layers.TimeDistributed(\n",
        "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))(\n",
        "            inputs)\n",
        "        model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))(model)\n",
        "    for _ in range(1):\n",
        "        model = tf.keras.layers.BatchNormalization()(model)  # Adding a normalization layer\n",
        "    model = tf.keras.layers.TimeDistributed(\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(model)\n",
        "    for _ in range(2):\n",
        "        model = tf.keras.layers.TimeDistributed(\n",
        "            tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(model)\n",
        "        model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))(model)\n",
        "    for _ in range(1):\n",
        "        model = tf.keras.layers.BatchNormalization()(model)  # Adding a normalization layer\n",
        "\n",
        "    model = tf.keras.layers.Reshape((sequence_length, -1))(inputs)\n",
        "    lstm_units = lstm_units1\n",
        "    # Create LSTM layers based on num_lstm_layers\n",
        "    for _ in range(num_lstm_layers):\n",
        "        model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_units1, return_sequences=True))(model)\n",
        "    # Add Temporal Attention mechanism\n",
        "    model = tf.keras.layers.Reshape((-1, lstm_units * 2))(model)\n",
        "    permute1 = tf.keras.layers.Permute((2, 1))(model)\n",
        "    attention_probs = tf.keras.layers.Dense(units=1, activation='softmax')(permute1)\n",
        "    permute2 = tf.keras.layers.Permute((2, 1))(attention_probs)\n",
        "    model = tf.keras.layers.Multiply()([model, permute2])\n",
        "    model = tf.keras.layers.Flatten()(model)\n",
        "    # Add fully connected layers and dropout layers\n",
        "    for _ in range(dense_layers):\n",
        "        model = tf.keras.layers.Dense(units=dense_units1)(model)\n",
        "    for _ in range(dropout_layers):\n",
        "        model = tf.keras.layers.Dropout(rate=dropout_rate1)(model)\n",
        "    num_classes = 1\n",
        "    outputs = tf.keras.layers.Dense(units=num_classes, activation='sigmoid')(model)\n",
        "    # Choose optimizers\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "    sgd = tf.keras.optimizers.SGD(lr=0.1, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=adam, loss=BinaryFocalLoss(gamma=gamma,pos_weight=pos_weight), metrics=['accuracy'])  # rmsprop adam\n",
        "    # Print the model structure\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def preprocess_sequence(sequence):\n",
        "    # Read all images and store them in a dictionary\n",
        "    images = {}\n",
        "    folder_path = \"residues32/IA\"\n",
        "    file_names = os.listdir(folder_path)\n",
        "\n",
        "    # Load and preprocess images\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        image = cv2.imread(file_path)\n",
        "        image = cv2.resize(image, (32, 32))  # Resize the image to 32x32\n",
        "        image = tf.cast(image, tf.float16) / 255.0  # Convert to float32 and normalize to [0, 1]\n",
        "        # Replace NaN with 0\n",
        "        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
        "        images[file_path[14:-4]] = image\n",
        "    def map_seq(input_str):\n",
        "        char_images = []  # 创建一个空列表\n",
        "        prev_index = None\n",
        "        # Iterate over each character in the input string\n",
        "        for index, char in enumerate(input_str):\n",
        "            # If the current character is \"x\", remember its index and exit the loop\n",
        "            if char == 'x':\n",
        "                prev_index = index-1\n",
        "                break\n",
        "        # If \"x\" is not found, remember the index of the last characte\n",
        "        if prev_index is None and len(input_str) > 0:\n",
        "            prev_index = len(input_str) - 1\n",
        "\n",
        "         # Iterate over each character in the input string\n",
        "        for n in range(len(input_str)):\n",
        "            if n == prev_index:\n",
        "                char = input_str[n]\n",
        "                image_key = char + '_C'\n",
        "\n",
        "                char_tensor = tf.convert_to_tensor(images.get(image_key))\n",
        "                char_images.append(char_tensor)\n",
        "            elif n == 0:\n",
        "                char = input_str[n]\n",
        "                image_key = char + '_N'\n",
        "                char_tensor = tf.convert_to_tensor(images.get(image_key))\n",
        "                char_images.append(char_tensor)\n",
        "\n",
        "            elif n != prev_index:\n",
        "                char = input_str[n]\n",
        "\n",
        "                char_tensor = tf.convert_to_tensor(images.get(char))\n",
        "                char_images.append(char_tensor)\n",
        "        char_images = np.array(char_images)\n",
        "\n",
        "        seq_frames = tf.stack(char_images, axis=0)\n",
        "        return seq_frames\n",
        "\n",
        "    input_seq = sequence.numpy().decode(\"utf-8\")\n",
        "    processed_data = []\n",
        "    seq_frames = map_seq(input_seq)\n",
        "    processed_data.append(seq_frames)\n",
        "    processed_data = tf.convert_to_tensor(processed_data)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def preprocess_seq(filename, max_length):\n",
        "    data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    sequences = data['sequence'].tolist()\n",
        "    labels = data['label'].tolist()\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    for seq, label in zip(sequences, labels):\n",
        "         # Strip trailing spaces and pad to specified length\n",
        "        seq = seq.strip().ljust(max_length, 'x')\n",
        "        processed_data.append((seq, label))  # Pack data and label into a tuple and add to list\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def get_max_length(filename1,filename2,max_length):\n",
        "\n",
        "    def count_max_length(data):\n",
        "        sequences = data['sequence'].tolist()\n",
        "        labels = data['label'].tolist()\n",
        "        max_length = 0\n",
        "        positive_sequences = []\n",
        "        negative_sequences = []\n",
        "        for seq, label in zip(sequences, labels):\n",
        "            if label == 1:\n",
        "                positive_sequences.append(seq)\n",
        "            else:\n",
        "                negative_sequences.append(seq)\n",
        "            max_length = max(max_length, len(seq))\n",
        "        return max_length\n",
        "    # Read each line from the file and associate it with the corresponding photo\n",
        "    data1 = pd.read_excel(filename1, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    data2 = pd.read_excel(filename2, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    max_length1 = count_max_length(data1)\n",
        "    max_length2 = count_max_length(data2)\n",
        "    if max_length1>max_length:\n",
        "        max_length =max_length1\n",
        "    if max_length2>max_length1:\n",
        "        max_length=max_length2\n",
        "\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "DJOemNKmpN94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following cell contains the main training code. After training, the results and hyperparameter settings are saved in the \"result.xlsx\" file."
      ],
      "metadata": {
        "id": "ytNrDrz6y4Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the logging level\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# Set the logging level to output only errors and warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "best_accuracy = 0.0\n",
        "# Generate unique random indices\n",
        "random_indices = random.sample(range(num_iterations), num_iterations)\n",
        "\n",
        "# Create an empty DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\n",
        "    'Iteration', 'gamma', 'pos_weight', 'lstm_units1', 'dense_units1',\n",
        "    'num_lstm_layers', 'dense_layers', 'dropout_rate1', 'dropout_layers', 'TP', 'FP', 'FN', 'TN', 'ACC',\n",
        "    'BACC', 'Sn', 'Sp', 'MCC', 'AUC', \"AUC_prime\", 'Preci sion', 'Recall', 'F1_score'\n",
        "])\n",
        "\n",
        "for i in random_indices:\n",
        "    gamma = gamma_values[i % len(gamma_values)]\n",
        "    pos_weight = pos_weight_values[i % len(pos_weight_values)]\n",
        "    lstm_units1 = lstm_units1_values[i % len(lstm_units1_values)]\n",
        "    dense_units1 = dense_units1_values[i % len(dense_units1_values)]\n",
        "    num_lstm_layers = num_lstm_layers_values[i % len(num_lstm_layers_values)]\n",
        "    dense_layers = dense_layers_values[i % len(dense_layers_values)]\n",
        "    dropout_rate1 = dropout_rate1_values[i % len(dropout_rate1_values)]\n",
        "    dropout_layers = dropout_layers_values[i % len(dropout_layers_values)]\n",
        "\n",
        "    max_length = 0\n",
        "    max_length = get_max_length(train_file, val_file, max_length)\n",
        "    print(max_length)\n",
        "    data_train =preprocess_seq(train_file,max_length)\n",
        "    data_test =preprocess_seq(val_file,max_length)\n",
        "\n",
        "    X = np.array([sequence for sequence, label in data_train])\n",
        "    y = np.array([label for sequence, label in data_train])\n",
        "    X_val = np.array([sequence for sequence, label in data_test])\n",
        "    y_val = np.array([label for sequence, label in data_test])\n",
        "\n",
        "    val_dataset =load_and_preprocess_data(X_val, y_val)\n",
        "    # Define 5-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Define the model saving path\n",
        "    model_path = 'best_model'+ str(i+1)+'.h5'\n",
        "\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "    # Define empty lists to store metrics for each k-fold training\n",
        "    accuracy_list = []\n",
        "    auc_list = []\n",
        "    bacc_list = []\n",
        "    sensitivity_list = []\n",
        "    specificity_list = []\n",
        "    mcc_list = []\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1,\n",
        "                                                     min_delta=1e-4, mode='min')\n",
        "    # Set up callbacks\n",
        "    custom_callback = CustomCallback()\n",
        "    lr_callback = LearningRateCallback()\n",
        "    callbacks = [custom_callback, lr_callback]\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Ensure the number of samples in the training and testing sets is a multiple of the batch size\n",
        "        train_samples = len(X_train) - (len(X_train) % 16)\n",
        "        test_samples = len(X_test) - (len(X_test) % 16)\n",
        "        X_train = X_train[:train_samples]\n",
        "        y_train = y_train[:train_samples]\n",
        "        X_test = X_test[:test_samples]\n",
        "        y_test = y_test[:test_samples]\n",
        "        assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "        train_dataset = load_and_preprocess_data(X_train, y_train)\n",
        "        test_dataset =load_and_preprocess_data(X_test,y_test)\n",
        "\n",
        "        model = train_model(train_dataset,test_dataset)\n",
        "        # Evaluate the model's performance on the validation set\n",
        "        val_loss, val_acc = model.evaluate(val_dataset)\n",
        "        print(\"The ACC of best model\"+str(val_acc))\n",
        "        print(\"The loss of best model\" + str(val_loss))\n",
        "        # Save the best model\n",
        "        if val_acc > best_accuracy:\n",
        "            best_model = model\n",
        "            best_accuracy = val_acc\n",
        "\n",
        "         # Initialize empty lists for y_true and y_pred_binary\n",
        "        y_true = []\n",
        "        y_pred_binary = []\n",
        "\n",
        "        # Iterate over the val_dataset and extract labels\n",
        "        for batch_inputs, batch_labels in val_dataset:\n",
        "            # Predict the output for each batch\n",
        "            batch_inputs = np.squeeze(batch_inputs, axis=1)\n",
        "            batch_predictions = model.predict(batch_inputs)\n",
        "            batch_predictions_binary = (batch_predictions > 0.5).astype(\"int32\")\n",
        "\n",
        "            # Append the labels and predictions of each batch to the lists\n",
        "            y_true.extend(batch_labels.numpy())\n",
        "            y_pred_binary.extend(batch_predictions_binary)\n",
        "\n",
        "        # Convert to NumPy arrays\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred_binary = np.array(y_pred_binary)\n",
        "\n",
        "        # Calculate various metrics\n",
        "        precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred_binary, zero_division=1)\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_binary)\n",
        "        cm = confusion_matrix(y_true, y_pred_binary)\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "        auc = roc_auc_score(y_true, y_pred_binary)\n",
        "        auc_prime = np.trapz(tpr, fpr)\n",
        "        sensitivity = recall_score(y_true, y_pred_binary)\n",
        "        specificity = recall_score(y_true, y_pred_binary, pos_label=0)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "        bacc = balanced_accuracy_score(y_true, y_pred_binary)\n",
        "        precision = precision_score(y_true, y_pred_binary)\n",
        "        recall = sensitivity\n",
        "        f1 = f1_score(y_true, y_pred_binary)\n",
        "\n",
        "        # Add the metrics of each k-fold training to the list\n",
        "        accuracy_list.append(accuracy)\n",
        "        auc_list.append(auc)\n",
        "        bacc_list.append(bacc)\n",
        "        sensitivity_list.append(sensitivity)\n",
        "        specificity_list.append(specificity)\n",
        "        mcc_list.append(mcc)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    # Calculate the standard deviation of each metric\n",
        "    accuracy_std = stdev(accuracy_list)\n",
        "    auc_std = stdev(auc_list)\n",
        "    bacc_std = stdev(bacc_list)\n",
        "    sensitivity_std = stdev(sensitivity_list)\n",
        "    specificity_std = stdev(specificity_list)\n",
        "    mcc_std = stdev(mcc_list)\n",
        "    precision_std = stdev(precision_list)\n",
        "    recall_std = stdev(recall_list)\n",
        "    f1_std = stdev(f1_list)\n",
        "\n",
        "\n",
        "    best_model.save('best_model'+str(i + 1)+'.h5')\n",
        "\n",
        "    val_accuracy = accuracy\n",
        "    # Update the best result\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_combination = {\n",
        "            'gamma': gamma,\n",
        "            'pos_weight': pos_weight,\n",
        "            'lstm_units': lstm_units1,\n",
        "            'dense_units': dense_units1,\n",
        "            'num_lstm_layers': num_lstm_layers,\n",
        "            'dense_layers': dense_layers,\n",
        "            'dropout_rate': dropout_rate1,\n",
        "        }\n",
        "\n",
        "    # Add the result to results_df\n",
        "    results_df = pd.concat([results_df, pd.DataFrame({\n",
        "        'Iteration': [i + 1],\n",
        "        'gamma': [gamma],\n",
        "        'pos_weight': [pos_weight],\n",
        "        'lstm_units1': [lstm_units1],\n",
        "        'dense_units1': [dense_units1],\n",
        "        'num_lstm_layers': [num_lstm_layers],\n",
        "        'dense_layers': [dense_layers],\n",
        "        'dropout_rate1': [dropout_rate1],\n",
        "        'dropout_layers': [dropout_layers],\n",
        "        'TP': [TP],\n",
        "        'FP': [FP],\n",
        "        'FN': [FN],\n",
        "        'TN': [TN],\n",
        "        'ACC': [accuracy],\n",
        "        'BACC': [bacc],\n",
        "        'Sn': [sensitivity],\n",
        "        'Sp': [specificity],\n",
        "        'MCC': [mcc],\n",
        "        'AUC': [auc],\n",
        "        'AUC_prime': [auc_prime],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1_score': [f1]\n",
        "    })], ignore_index=True)\n",
        "\n",
        "    # 将DataFrame写入Excel文件\n",
        "\n",
        "    results_df.to_excel(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "\n",
        "# 打印最佳结果的组合和最佳准确率\n",
        "print(f\"Best Combination: {best_combination}\")\n",
        "print(f\"Best Accuracy: {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade62633-c30c-43c0-900b-f93a61e447d2",
        "id": "trGdDdlJvdB2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集中字符最大长度: 180\n"
          ]
        }
      ]
    }
  ]
}