{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Environment and Working Directory Configuration"
      ],
      "metadata": {
        "id": "MHAxMyJUyDbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVsO1WmeDb-3"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==2.10.0\n",
        "!pip install cudnn==8.4.1\n",
        "!pip install cudatoolkit==11.8.0\n",
        "!pip install pillow\n",
        "!pip install scikit-learn\n",
        "!pip install openpyxl\n",
        "!pip install opencv\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pP0CGIqtDfG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Change to the target directory\n",
        "os.chdir('/content/drive/MyDrive/rdkit')\n",
        "# Print the current working directory\n",
        "print(\"The current working directory：\", os.getcwd())"
      ],
      "metadata": {
        "id": "qUCjKaV_DeEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Prediction File Path Configuration"
      ],
      "metadata": {
        "id": "f0kn8aGYyImz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'Dataset/23_Antioxidant/best_model3.h5'  # Path to load the model\n",
        "\n",
        "prediction_file = 'Dataset/23_Antioxidant/test.xlsx'  # Path to the prediction file\n",
        "\n",
        "results_output_path = 'results/predictions.csv'  # Set the output filename for prediction results (CSV format)"
      ],
      "metadata": {
        "id": "jkQQCYwBERiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_sequence(sequence):\n",
        "    # Read all images and store them in a dictionary\n",
        "    images = {}\n",
        "    folder_path = \"residues32/IA\"\n",
        "    file_names = os.listdir(folder_path)\n",
        "\n",
        "    # Load and preprocess images\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        image = cv2.imread(file_path)\n",
        "        image = cv2.resize(image, (32, 32))\n",
        "        image = tf.cast(image, tf.float16) / 255.0\n",
        "        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
        "        images[file_path[14:-4]] = image\n",
        "    def map_seq(input_str):\n",
        "\n",
        "        char_images = []\n",
        "        prev_index = None\n",
        "        for index, char in enumerate(input_str):\n",
        "\n",
        "            if char == 'x':\n",
        "                prev_index = index-1\n",
        "                break\n",
        "\n",
        "        if prev_index is None and len(input_str) > 0:\n",
        "            prev_index = len(input_str) - 1\n",
        "\n",
        "        for n in range(len(input_str)):\n",
        "            if n == prev_index:\n",
        "                char = input_str[n]\n",
        "                image_key = char + '_C'\n",
        "\n",
        "                char_tensor = tf.convert_to_tensor(images.get(image_key))\n",
        "                char_images.append(char_tensor)\n",
        "            elif n == 0:\n",
        "                char = input_str[n]\n",
        "                image_key = char + '_N'\n",
        "                char_tensor = tf.convert_to_tensor(images.get(image_key))\n",
        "                char_images.append(char_tensor)\n",
        "\n",
        "            elif n != prev_index:\n",
        "                char = input_str[n]\n",
        "\n",
        "                char_tensor = tf.convert_to_tensor(images.get(char))\n",
        "                char_images.append(char_tensor)\n",
        "        char_images = np.array(char_images)\n",
        "\n",
        "        seq_frames = tf.stack(char_images, axis=0)\n",
        "        return seq_frames\n",
        "\n",
        "    input_seq = sequence.numpy().decode(\"utf-8\")\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    seq_frames = map_seq(input_seq)\n",
        "    processed_data.append(seq_frames)\n",
        "    processed_data = tf.convert_to_tensor(processed_data)\n",
        "    return processed_data\n",
        "\n",
        "def preprocess_seq(filename, max_length):\n",
        "    data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    sequences = data['sequence'].tolist()\n",
        "    labels = data['label'].tolist()\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    for seq, label in zip(sequences, labels):\n",
        "\n",
        "        seq = seq.strip().ljust(max_length, 'x')\n",
        "        processed_data.append((seq, label))\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def get_max_length(filename1,filename2,max_length):\n",
        "\n",
        "    def count_max_length(data):\n",
        "        sequences = data['sequence'].tolist()\n",
        "        labels = data['label'].tolist()\n",
        "        max_length = 0\n",
        "        positive_sequences = []\n",
        "        negative_sequences = []\n",
        "        for seq, label in zip(sequences, labels):\n",
        "            if label == 1:\n",
        "                positive_sequences.append(seq)\n",
        "            else:\n",
        "                negative_sequences.append(seq)\n",
        "            max_length = max(max_length, len(seq))\n",
        "        return max_length\n",
        "\n",
        "    data1 = pd.read_excel(filename1, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    data2 = pd.read_excel(filename2, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "    max_length1 = count_max_length(data1)\n",
        "    max_length2 = count_max_length(data2)\n",
        "    if max_length1>max_length:\n",
        "        max_length =max_length1\n",
        "    if max_length2>max_length1:\n",
        "        max_length=max_length2\n",
        "    return max_length\n",
        "def load_and_preprocess_data(sequences, labels, batch_size=16):\n",
        "\n",
        "    sequences = tf.constant(sequences, dtype=tf.string)\n",
        "    labels = tf.constant(labels, dtype=tf.int32)\n",
        "\n",
        "    sequence_dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "    labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((sequence_dataset, labels_dataset))\n",
        "\n",
        "    def map_fn(sequence, label):\n",
        "\n",
        "        processed_sequence = tf.py_function(preprocess_sequence, [sequence], tf.float32)\n",
        "        return processed_sequence,sequence, label\n",
        "\n",
        "    dataset = dataset.map(lambda sequence, label: map_fn(sequence, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def read_sequences(filename):\n",
        "    \"\"\"\n",
        "    自动识别文件格式并读取其中的序列\n",
        "    Args:\n",
        "      filename: filename\n",
        "\n",
        "    Returns:\n",
        "      sequences: list of sequences\n",
        "    \"\"\"\n",
        "    _, ext = os.path.splitext(filename)\n",
        "    if ext == \".fasta\":\n",
        "        with open(filename, \"r\") as f:\n",
        "            sequences = list(SeqIO.parse(f, \"fasta\"))\n",
        "        return [str(record.seq) for record in sequences]\n",
        "    elif ext in [\".txt\"]:\n",
        "        with open(filename, \"r\") as f:\n",
        "            sequences = f.readlines()\n",
        "        return [seq.strip() for seq in sequences]\n",
        "    elif ext == \".xlsx\":\n",
        "        data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n",
        "        return data['Sequence'].tolist()\n",
        "    elif ext in [\".csv\"]:\n",
        "        data = pd.read_csv(filename, na_filter=False)\n",
        "        return data['Sequence'].tolist()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format: {}\".format(ext))\n"
      ],
      "metadata": {
        "id": "D0sBhB77DjkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "model.summary()\n",
        "# Get the model input shape to determine max_length\n",
        "input_shape = model.input_shape\n",
        "max_length = input_shape[1]\n",
        "sequences = read_sequences(filename)\n",
        "preprocess_predictions = preprocess_seq(sequences, max_length)\n",
        "\n",
        "batch_frame = np.vstack([preprocess_sequence(seq) for seq in preprocess_predictions])\n",
        "batch_predictions = model.predict(batch_frame)\n",
        "\n",
        "batch_predictions_binary = (batch_predictions > 0.5).astype(\"int32\")\n",
        "\n",
        "with open(results_output_path, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    if csvfile.tell() == 0:\n",
        "        writer.writerow([\"Sequence\", \"Label\", \"Prediction\"])\n",
        "    for seq, predictions in zip(sequences, batch_predictions):\n",
        "        seq_str = f'{seq}'\n",
        "        if len(seq) > max_length:\n",
        "            label = 'out of max length'\n",
        "            predictions_str = 'out of max length'\n",
        "            writer.writerow([seq_str, label, predictions_str])\n",
        "        else:\n",
        "            label = 0 if predictions < 0.5 else 1\n",
        "            prediction_value = predictions[0]\n",
        "            predictions_str = f\"{prediction_value:.6f}\"\n",
        "            writer.writerow([seq_str, label, predictions_str])"
      ],
      "metadata": {
        "id": "eRLoJgCsDta-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}