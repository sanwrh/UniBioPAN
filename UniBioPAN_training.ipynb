{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14uVX4WyT1QxajsIdBx_Wyfsl7OfkBY-_","timestamp":1710417452492}],"gpuType":"V100","mount_file_id":"14uVX4WyT1QxajsIdBx_Wyfsl7OfkBY-_","authorship_tag":"ABX9TyMs56WBQ9GQBN4jwyEds/W8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow-gpu==2.10.0\n","!pip install cudnn==8.4.1\n","!pip install cudatoolkit==11.8.0\n","!pip install pillow\n","!pip install scikit-learn\n","!pip install openpyxl\n","!pip install opencv\n","!pip install pandas\n","!pip install matplotlib"],"metadata":{"id":"cgonus2HlQk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"H8HMVUTCYHJw"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRyIVWSqj7a2","executionInfo":{"status":"ok","timestamp":1684813265470,"user_tz":-480,"elapsed":2698,"user":{"displayName":"王定春","userId":"14583378729649577997"}},"outputId":"320a2e81-1041-48e8-8629-527e44286930"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.10.0\n","['/usr/local/lib/python3.10/dist-packages/keras/api/_v2', '/usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/api/_v2', '/usr/local/lib/python3.10/dist-packages/tensorboard/summary/_tf', '/usr/local/lib/python3.10/dist-packages/tensorflow', '/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2']\n"]}],"source":["import tensorflow as tf\n","tf.__version__        # 此命令为获取安装的tensorflow版本\n","print(tf.__version__) # 输出版本\n","tf.__path__\t\t\t   #查看tensorflow安装路径\n","print(tf.__path__)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"lhvTV0J2nrTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# 切换到目标目录\n","os.chdir('/content/drive/MyDrive/rdkit')\n","# 打印当前工作目录\n","print(\"当前工作目录：\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXlL44tnoJtf","executionInfo":{"status":"ok","timestamp":1684823786466,"user_tz":-480,"elapsed":430,"user":{"displayName":"王定春","userId":"14583378729649577997"}},"outputId":"3127ee32-24a3-49c9-d11c-5029f1ae5808"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["当前工作目录： /content/drive/MyDrive/rdkit\n"]}]},{"cell_type":"markdown","source":["## All parameters and file paths are set in the next cell."],"metadata":{"id":"g3ldG0vSzTYZ"}},{"cell_type":"code","source":["# Focal Loss Parameters\n","gamma_values = [1]   # Typically between 1.0 and 5.0, increase the focus on samples that are more difficult to classify.\n","pos_weight_values = [1]  # Values less than 1.0 emphasize positive samples in imbalanced datasets where positive samples are rare.\n","# BiLSTM Layer Parameters\n","lstm_units1_values = [32]\n","num_lstm_layers_values = [1]\n","# Dense Layer Parameters\n","dense_units1_values = [32]\n","dense_layers_values = [3]\n","# Dropout Layer Parameters\n","dropout_rate1_values = [0.3]\n","dropout_layers_values = [1]\n","\n","# Set the number of random iterations\n","num_iterations = 10\n","# training set and independent test set path setting\n","train_file = 'Dataset/24_IL-6/IL6_train.xlsx'\n","val_file = 'Dataset/24_IL-6/IL6_test.xlsx'"],"metadata":{"id":"YT8aMrUSyD7J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The following cell imports the necessary modules and defines custom function for training the model. The code does not require any modification."],"metadata":{"id":"UMcff4l1wKqU"}},{"cell_type":"code","source":["import cv2\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, roc_auc_score\n","from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n","from preprocess_data_test import preprocess_seq, get_max_length, preprocess_sequence\n","from tensorflow.python.keras.callbacks import EarlyStopping\n","from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, balanced_accuracy_score, roc_curve, \\\n","    matthews_corrcoef, precision_score, recall_score, f1_score\n","import math\n","from statistics import stdev\n","import random\n","import logging\n","from focal_loss import BinaryFocalLoss\n","\n","def map_fn(sequence, label):\n","    processed_sequence = tf.py_function(preprocess_sequence, [sequence], tf.float32)\n","    return processed_sequence, label\n","def load_and_preprocess_data(sequences, labels):\n","    sequences = tf.constant(sequences, dtype=tf.string)\n","    labels = tf.constant(labels, dtype=tf.float32)\n","\n","    # 创建 tf.data.Dataset，并使用 map 应用处理函数\n","    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n","    dataset = dataset.map(lambda sequence, label: map_fn(sequence, label), num_parallel_calls=tf.data.AUTOTUNE)\n","    #dataset = dataset.shuffle(buffer_size=len(sequences)).cache()\n","    dataset = dataset.cache()\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(16)\n","    return dataset\n","\n","# 自定义回调函数用于打印指标\n","class PrintMetricsCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(f\"Epoch {epoch + 1}:\")\n","        print(f\"  loss: {logs['loss']:.4f}\")\n","        print(f\"  accuracy: {logs['accuracy']:.4f}\")\n","        print(f\"  val_loss: {logs['val_loss']:.4f}\")\n","        print(f\"  val_accuracy: {logs['val_accuracy']:.4f}\")\n","        print(f\"  learning_rate: {tf.keras.backend.get_value(model.optimizer.lr):.6f}\")\n","\n","class Attention(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(Attention, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.context_vector = self.add_weight(shape=(input_shape[-1], 1), initializer='glorot_uniform',\n","                                              trainable=True)\n","\n","    def call(self, inputs):\n","        # Compute attention scores\n","        attention_scores = tf.matmul(inputs, self.context_vector)\n","        attention_scores = tf.squeeze(attention_scores, axis=-1)\n","        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n","\n","        # Apply attention weights to inputs\n","        weighted_inputs = tf.multiply(inputs, tf.expand_dims(attention_weights, axis=-1))\n","        context_vector = tf.reduce_sum(weighted_inputs, axis=1)\n","\n","        return context_vector, attention_weights\n","\n","# 创建一个回调函数，用于在每个epoch结束时打印相关信息\n","class CustomCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(\n","            f\"Epoch {epoch + 1} - Train Loss: {logs['loss']:.4f}, Train Accuracy: {logs['accuracy']:.4f}, Test Loss: {logs['val_loss']:.4f}, Test Accuracy: {logs['val_accuracy']:.4f}\")\n","\n","# 定义学习率回调函数，用于在每个epoch结束时打印学习率\n","class LearningRateCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        lr = self.model.optimizer.lr.numpy()\n","        print(f\"Epoch {epoch + 1} - Learning Rate: {lr:.6f}\")\n","# 在模型定义后，进行编译之前，定义Focal  Loss类\n","class FocalLoss(tf.keras.losses.Loss):\n","    def __init__(self, gamma=2, alpha=0.5):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","\n","    def call(self, y_true, y_pred):\n","        epsilon = tf.keras.backend.epsilon()\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n","        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n","        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n","        modulating_factor = tf.pow(1.0 - p_t, self.gamma)\n","        focal_loss = -alpha_factor * modulating_factor * tf.math.log(p_t)\n","        return focal_loss\n","\n","def step_decay(epoch):\n","    initial_lrate = 0.1\n","    drop = 0.6\n","    epochs_drop = 3.0\n","    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n","    return lrate\n","    # Define the training process\n","def train_model(train_dataset, test_dataset):\n","    strategy = tf.distribute.MirroredStrategy(devices=['GPU:0', 'GPU:1'])\n","    with strategy.scope():\n","        # Define ModelCheckpoint callback\n","        checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True,\n","                                                        mode='max', verbose=0)\n","        # 定义 EarlyStopping 回调函数\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n","\n","        learning_rate = 0.0001\n","        model = create_model()\n","        model.optimizer.lr.assign(learning_rate)\n","        print_learning_rate = tf.keras.callbacks.LambdaCallback(\n","            on_epoch_begin=lambda epoch, logs: print(\n","                f\"Learning Rate: {tf.keras.backend.get_value(model.optimizer.lr):.6f}\"))\n","        callbacks = [reduce_lr, early_stopping, print_learning_rate,checkpoint]\n","        model.fit(train_dataset, epochs=400, batch_size=16, validation_data=test_dataset,\n","                  callbacks=callbacks, verbose=2)\n","    return model\n","def create_model():\n","    learning_rate = 0.0001\n","    momentum = 0.5\n","    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n","    # 输入序列的长度\n","    sequence_length = max_length\n","    input_shape = (sequence_length, 32, 32, 3)\n","    inputs = tf.keras.Input(shape=input_shape)\n","    # model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (1, 1), activation='relu', padding='same'))(\n","    initializer = tf.keras.initializers.HeNormal(seed=123456)\n","    model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))(\n","        inputs)\n","    for _ in range(2):\n","        model = tf.keras.layers.TimeDistributed(\n","            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))(\n","            inputs)\n","        model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))(model)\n","    for _ in range(1):\n","        model = tf.keras.layers.BatchNormalization()(model)  # Adding a normalization layer\n","    model = tf.keras.layers.TimeDistributed(\n","        tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(model)\n","    for _ in range(2):\n","        model = tf.keras.layers.TimeDistributed(\n","            tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(model)\n","        model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))(model)\n","    for _ in range(1):\n","        model = tf.keras.layers.BatchNormalization()(model)  # Adding a normalization layer\n","\n","    model = tf.keras.layers.Reshape((sequence_length, -1))(inputs)\n","    lstm_units = lstm_units1\n","    # 根据num_lstm_layers创建指定层数的LSTM层\n","    for _ in range(num_lstm_layers):\n","        model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_units1, return_sequences=True))(model)\n","    # 添加 Temporal Attention 机制\n","    model = tf.keras.layers.Reshape((-1, lstm_units * 2))(model)\n","    permute1 = tf.keras.layers.Permute((2, 1))(model)\n","    attention_probs = tf.keras.layers.Dense(units=1, activation='softmax')(permute1)\n","    permute2 = tf.keras.layers.Permute((2, 1))(attention_probs)\n","    model = tf.keras.layers.Multiply()([model, permute2])\n","    model = tf.keras.layers.Flatten()(model)\n","    # 添加全连接层和dropout层\n","    for _ in range(dense_layers):\n","        model = tf.keras.layers.Dense(units=dense_units1)(model)\n","    for _ in range(dropout_layers):\n","        model = tf.keras.layers.Dropout(rate=dropout_rate1)(model)\n","    num_classes = 1\n","    outputs = tf.keras.layers.Dense(units=num_classes, activation='sigmoid')(model)\n","    # 创建模型\n","    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n","    sgd = tf.keras.optimizers.SGD(lr=0.1, momentum=momentum, decay=0.0, nesterov=False)\n","    rmsprop = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","    adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    # 编译模型\n","    model.compile(optimizer=adam, loss=BinaryFocalLoss(gamma=gamma,pos_weight=pos_weight), metrics=['accuracy'])  # rmsprop adam\n","    # 打印模型结构\n","    model.summary()\n","    return model\n","def preprocess_sequence(sequence):\n","    # 读取所有照片并将它们存储在一个字典中\n","    images = {}\n","    folder_path = \"residues32/IA\"\n","    file_names = os.listdir(folder_path)\n","\n","    # 加载和预处理图像\n","    for file_name in file_names:\n","        file_path = os.path.join(folder_path, file_name)\n","        image = cv2.imread(file_path)\n","        image = cv2.resize(image, (32, 32))  # 调整图像大小为 32x32\n","        image = tf.cast(image, tf.float32) / 255.0  # 转换为 float32 并标准化到 [0, 1] 范围内\n","        # # 应用 Z-分数标准化（减去均值并除以标准差）\n","        # mean = tf.math.reduce_mean(image)\n","        # std = tf.math.reduce_std(image)\n","        # image = (image - mean) / std\n","\n","        # 将NaN替换为0\n","        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n","        images[file_path[14:-4]] = image\n","    def map_seq(input_str):\n","        # print(input_str)\n","        # # 查看图像的形状以调试\n","        char_images = []  # 创建一个空列表\n","        prev_index = None\n","        # 遍历输入字符串中的每个字符\n","        for index, char in enumerate(input_str):\n","            # 如果当前字符是 \"x\"，记住它的索引并退出循环\n","            if char == 'x':\n","                prev_index = index-1\n","                break\n","        # 如果没有找到 \"x\"，则记住最后一个字符的索引\n","        if prev_index is None and len(input_str) > 0:\n","            prev_index = len(input_str) - 1\n","\n","        # 遍历输入字符串中的每个字符\n","        for n in range(len(input_str)):\n","            # for char in input_str[n:n+1]:\n","            if n == prev_index:\n","                char = input_str[n]\n","                image_key = char + '_C'\n","                #print(image_key)\n","                char_tensor = tf.convert_to_tensor(images.get(image_key))\n","                char_images.append(char_tensor)\n","            elif n == 0:\n","                char = input_str[n]\n","                image_key = char + '_N'\n","                char_tensor = tf.convert_to_tensor(images.get(image_key))\n","                char_images.append(char_tensor)\n","            # 检查字符是否在images字典中\n","            elif n != prev_index:\n","                char = input_str[n]\n","                #print(char)\n","                # 如果在images字典中，将对应的图像转换为Tensor并添加到列表中\n","                char_tensor = tf.convert_to_tensor(images.get(char))\n","                char_images.append(char_tensor)\n","        char_images = np.array(char_images)\n","\n","        seq_frames = tf.stack(char_images, axis=0)\n","        return seq_frames\n","\n","    input_seq = sequence.numpy().decode(\"utf-8\")\n","    #print(\"input sequence\"+input_seq)\n","    processed_data = []\n","    # for seq in input_seq:\n","    #     print(seq)\n","    seq_frames = map_seq(input_seq)\n","    processed_data.append(seq_frames)\n","    processed_data = tf.convert_to_tensor(processed_data)\n","    #processed_data = tf.squeeze(processed_data, axis=1)\n","    return processed_data\n","\n","def preprocess_seq(filename, max_length):\n","    data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n","    sequences = data['sequence'].tolist()\n","    labels = data['label'].tolist()\n","    # print(len(sequences))\n","    # print(len(labels))\n","    processed_data = []\n","\n","    for seq, label in zip(sequences, labels):\n","        # 移除行尾空格并填充到指定长度\n","        seq = seq.strip().ljust(max_length, 'x')\n","        processed_data.append((seq, label))  # 将数据和标签打包成一个元组并添加到列表中\n","    #print(processed_data)\n","    return processed_data\n","\n","def get_max_length(filename1,filename2,max_length):\n","\n","    def count_max_length(data):\n","        sequences = data['sequence'].tolist()\n","        labels = data['label'].tolist()\n","        max_length = 0\n","        positive_sequences = []\n","        negative_sequences = []\n","        for seq, label in zip(sequences, labels):\n","            if label == 1:\n","                positive_sequences.append(seq)\n","            else:\n","                negative_sequences.append(seq)\n","            max_length = max(max_length, len(seq))\n","        return max_length\n","    # 从文件中读取每一行并将其与相应的照片相关联\n","    data1 = pd.read_excel(filename1, engine='openpyxl', keep_default_na=False, na_values=[''])  # 指定engine为'openpyxl'或'xlrd'\n","    data2 = pd.read_excel(filename2, engine='openpyxl', keep_default_na=False, na_values=[''])\n","    max_length1 = count_max_length(data1)\n","    max_length2 = count_max_length(data2)\n","    if max_length1>max_length:\n","        max_length =max_length1\n","    if max_length2>max_length1:\n","        max_length=max_length2\n","    print(\"数据集中字符最大长度:\", max_length)\n","    return max_length"],"metadata":{"id":"DJOemNKmpN94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The following cell contains the main training code. After training, the results and hyperparameter settings are saved in the \"result.xlsx\" file."],"metadata":{"id":"ytNrDrz6y4Qw"}},{"cell_type":"code","source":[],"metadata":{"id":"erAi4Z80qCQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 设置日志级别\n","logging.basicConfig(level=logging.INFO)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","# 设置日志级别为只输出错误信息和警告\n","tf.get_logger().setLevel('ERROR')\n","best_accuracy = 0.0\n","best_combination = {}\n","# 生成不重复的随机索引\n","random_indices = random.sample(range(num_iterations), num_iterations)\n","\n","# 创建一个空的DataFrame来存储结果\n","results_df = pd.DataFrame(columns=[\n","    'Iteration', 'gamma', 'pos_weight', 'lstm_units1', 'dense_units1',\n","    'num_lstm_layers', 'dense_layers', 'dropout_rate1', 'dropout_layers', 'TP', 'FP', 'FN', 'TN', 'ACC',\n","    'BACC', 'Sn', 'Sp', 'MCC', 'AUC', \"AUC_prime\", 'Preci sion', 'Recall', 'F1_score'\n","])\n","\n","for i in random_indices:\n","    gamma = gamma_values[i % len(gamma_values)]\n","    pos_weight = pos_weight_values[i % len(pos_weight_values)]\n","    lstm_units1 = lstm_units1_values[i % len(lstm_units1_values)]\n","    dense_units1 = dense_units1_values[i % len(dense_units1_values)]\n","    num_lstm_layers = num_lstm_layers_values[i % len(num_lstm_layers_values)]\n","    dense_layers = dense_layers_values[i % len(dense_layers_values)]\n","    dropout_rate1 = dropout_rate1_values[i % len(dropout_rate1_values)]\n","    dropout_layers = dropout_layers_values[i % len(dropout_layers_values)]\n","\n","    max_length = 0\n","    max_length = get_max_length(train_file, val_file, max_length)\n","    print(max_length)\n","    data_train =preprocess_seq(train_file,max_length)\n","    data_test =preprocess_seq(val_file,max_length)\n","\n","    X = np.array([sequence for sequence, label in data_train])\n","    y = np.array([label for sequence, label in data_train])\n","    X_val = np.array([sequence for sequence, label in data_test])\n","    y_val = np.array([label for sequence, label in data_test])\n","\n","    val_dataset =load_and_preprocess_data(X_val, y_val)\n","    # 定义五折交叉验证\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    # 定义模型保存路径\n","    model_path = 'best_model'+ str(i+1)+'.h5'\n","    # best_model = model = tf.keras.models.load_model('best_model.h5')\n","    best_model = None\n","    best_accuracy = 0\n","    # 定义空列表用于存储每个k折训练的指标\n","    accuracy_list = []\n","    auc_list = []\n","    bacc_list = []\n","    sensitivity_list = []\n","    specificity_list = []\n","    mcc_list = []\n","    precision_list = []\n","    recall_list = []\n","    f1_list = []\n","\n","    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1,\n","                                                     min_delta=1e-4, mode='min')\n","    # 设置回调函数\n","    custom_callback = CustomCallback()\n","    lr_callback = LearningRateCallback()\n","    callbacks = [custom_callback, lr_callback]\n","\n","    for train_index, test_index in skf.split(X, y):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        # 确保训练集和测试集的样本数量是批量大小的整数倍\n","        train_samples = len(X_train) - (len(X_train) % 16)\n","        test_samples = len(X_test) - (len(X_test) % 16)\n","        X_train = X_train[:train_samples]\n","        y_train = y_train[:train_samples]\n","        X_test = X_test[:test_samples]\n","        y_test = y_test[:test_samples]\n","        assert X_test.shape[0] == y_test.shape[0]\n","\n","        train_dataset = load_and_preprocess_data(X_train, y_train)\n","        test_dataset =load_and_preprocess_data(X_test,y_test)\n","\n","        model = train_model(train_dataset,test_dataset)\n","        # 评估模型在验证集上的性能\n","        val_loss, val_acc = model.evaluate(val_dataset)\n","        print(\"最佳模型的ACC\"+str(val_acc))\n","        print(\"最佳模型的loss\" + str(val_loss))\n","        # 保存最佳模型\n","        if val_acc > best_accuracy:\n","            best_model = model\n","            best_accuracy = val_acc\n","\n","        # 初始化空的y_true和y_pred_binary列表\n","        y_true = []\n","        y_pred_binary = []\n","\n","        # 遍历val_dataset并提取标签\n","        for batch_inputs, batch_labels in val_dataset:\n","            # 预测每个批次的输出\n","            batch_inputs = np.squeeze(batch_inputs, axis=1)\n","            batch_predictions = model.predict(batch_inputs)\n","            batch_predictions_binary = (batch_predictions > 0.5).astype(\"int32\")\n","\n","            # 将每个批次的标签和预测值添加到列表中\n","            y_true.extend(batch_labels.numpy())\n","            y_pred_binary.extend(batch_predictions_binary)\n","            # print(y_true)\n","\n","        # 转换为NumPy数组\n","        y_true = np.array(y_true)\n","        y_pred_binary = np.array(y_pred_binary)\n","\n","        # 计算各项指标\n","        precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred_binary, zero_division=1)\n","        fpr, tpr, thresholds = roc_curve(y_true, y_pred_binary)\n","        cm = confusion_matrix(y_true, y_pred_binary)\n","        TN, FP, FN, TP = cm.ravel()\n","        accuracy = accuracy_score(y_true, y_pred_binary)\n","        auc = roc_auc_score(y_true, y_pred_binary)\n","        auc_prime = np.trapz(tpr, fpr)\n","        sensitivity = recall_score(y_true, y_pred_binary)\n","        specificity = recall_score(y_true, y_pred_binary, pos_label=0)\n","        mcc = matthews_corrcoef(y_true, y_pred_binary)\n","        bacc = balanced_accuracy_score(y_true, y_pred_binary)\n","        precision = precision_score(y_true, y_pred_binary)\n","        recall = sensitivity\n","        f1 = f1_score(y_true, y_pred_binary)\n","\n","        # 将每个k折训练的指标添加到列表中\n","        accuracy_list.append(accuracy)\n","        auc_list.append(auc)\n","        bacc_list.append(bacc)\n","        sensitivity_list.append(sensitivity)\n","        specificity_list.append(specificity)\n","        mcc_list.append(mcc)\n","        precision_list.append(precision)\n","        recall_list.append(recall)\n","        f1_list.append(f1)\n","\n","    # 计算各项指标的标准差\n","    accuracy_std = stdev(accuracy_list)\n","    auc_std = stdev(auc_list)\n","    bacc_std = stdev(bacc_list)\n","    sensitivity_std = stdev(sensitivity_list)\n","    specificity_std = stdev(specificity_list)\n","    mcc_std = stdev(mcc_list)\n","    precision_std = stdev(precision_list)\n","    recall_std = stdev(recall_list)\n","    f1_std = stdev(f1_list)\n","\n","    print('Best model accuracy:', best_accuracy)\n","    # 打印各项指标及其标准差\n","    print('Accuracy:', accuracy_list)\n","    print('Accuracy (std):', accuracy_std)\n","    print('AUC:', auc_list)\n","    print('AUC (std):', auc_std)\n","    print('Balanced Accuracy:', bacc_list)\n","    print('Balanced Accuracy (std):', bacc_std)\n","    print('Sensitivity:', sensitivity_list)\n","    print('Sensitivity (std):', sensitivity_std)\n","    print('Specificity:', specificity_list)\n","    print('Specificity (std):', specificity_std)\n","    print('MCC:', mcc_list)\n","    print('MCC (std):', mcc_std)\n","    print('Precision:', precision_list)\n","    print('Precision (std):', precision_std)\n","    print('Recall:', recall_list)\n","    print('Recall (std):', recall_std)\n","    print('F1:', f1_list)\n","    print('F1 (std):', f1_std)\n","    best_model.save('best_model'+str(i + 1)+'.h5')\n","    # 在这里执行您的代码，使用当前的参数组合\n","    print(f\"Iteration {i + 1}/{num_iterations}: \"\n","          f\"gamma={gamma}, pos_weight={pos_weight}, \"\n","          f\"lstm_units1={lstm_units1}, dense_units1={dense_units1}, \"\n","          f\"num_lstm_layers={num_lstm_layers}, dense_layers={dense_layers}, \"\n","          f\"dropout_rate1={dropout_rate1},\"\n","          f\"dropout_layers={dropout_layers}\")\n","    # 打印结果\n","    print(\"True Positive (TP):\", TP)\n","    print(\"False Positive (FP):\", FP)\n","    print(\"False Negative (FN):\", FN)\n","    print(\"True Negative (TN):\", TN)\n","    print(\"Accuracy (ACC):\", accuracy)\n","    print(\"Balanced Accuracy (BACC):\", bacc)\n","    print(\"Sensitivity (Sn):\", sensitivity)\n","    print(\"Specificity (Sp):\", specificity)\n","    print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n","    print(\"Area Under the Curve (AUC):\", auc)\n","    print(\"AUC' (AUC prime):\", auc_prime)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1-score:\", f1)\n","    val_accuracy = accuracy\n","    # 收集val_accuracy\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy\n","        best_combination = {\n","            'gamma': gamma,\n","            'pos_weight': pos_weight,\n","            'lstm_units': lstm_units1,\n","            'dense_units': dense_units1,\n","            'num_lstm_layers': num_lstm_layers,\n","            'dense_layers': dense_layers,\n","            'dropout_rate': dropout_rate1,\n","        }\n","    #print(pd.__version__)\n","    #print(type(results_df))# 将参数和评估指标添加到DataFrame中\n","    results_df = pd.concat([results_df, pd.DataFrame({\n","        'Iteration': [i + 1],\n","        'gamma': [gamma],\n","        'pos_weight': [pos_weight],\n","        'lstm_units1': [lstm_units1],\n","        'dense_units1': [dense_units1],\n","        'num_lstm_layers': [num_lstm_layers],\n","        'dense_layers': [dense_layers],\n","        'dropout_rate1': [dropout_rate1],\n","        'dropout_layers': [dropout_layers],\n","        'TP': [TP],\n","        'FP': [FP],\n","        'FN': [FN],\n","        'TN': [TN],\n","        'ACC': [accuracy],\n","        'BACC': [bacc],\n","        'Sn': [sensitivity],\n","        'Sp': [specificity],\n","        'MCC': [mcc],\n","        'AUC': [auc],\n","        'AUC_prime': [auc_prime],\n","        'Precision': [precision],\n","        'Recall': [recall],\n","        'F1_score': [f1]\n","    })], ignore_index=True)\n","\n","    # 将DataFrame写入Excel文件\n","    output_path = 'results.xlsx'\n","    results_df.to_excel(output_path, index=False)\n","    print(f\"Results saved to {output_path}\")\n","\n","# 打印最佳结果的组合和最佳准确率\n","print(f\"Best Combination: {best_combination}\")\n","print(f\"Best Accuracy: {best_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ade62633-c30c-43c0-900b-f93a61e447d2","id":"trGdDdlJvdB2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["数据集中字符最大长度: 180\n"]}]}]}