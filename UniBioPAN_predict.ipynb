{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G5bP97xbYc5KxNu_dfUewl3rJkRjkg9o","timestamp":1710473040120}],"authorship_tag":"ABX9TyN0aRTo0KvBBw/5A19udoVN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GVsO1WmeDb-3"},"outputs":[],"source":["!pip install tensorflow-gpu==2.10.0\n","!pip install cudnn==8.4.1\n","!pip install cudatoolkit==11.8.0\n","!pip install pillow\n","!pip install scikit-learn\n","!pip install openpyxl\n","!pip install opencv\n","!pip install pandas\n","!pip install matplotlib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pP0CGIqtDfG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# 切换到目标目录\n","os.chdir('/content/drive/MyDrive/rdkit')\n","# 打印当前工作目录\n","print(\"当前工作目录：\", os.getcwd())"],"metadata":{"id":"qUCjKaV_DeEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = 'Dataset/23_Antioxidant/best_model3.h5'\n","\n","prediction_file = 'Dataset/23_Antioxidant/test.xlsx'\n","\n","output_file_path = 'predictions.csv'"],"metadata":{"id":"jkQQCYwBERiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import re\n","\n","def preprocess_sequence(sequence):\n","    # 读取所有照片并将它们存储在一个字典中\n","    images = {}\n","    folder_path = \"residues32/IA\"\n","    file_names = os.listdir(folder_path)\n","\n","    # 加载和预处理图像\n","    for file_name in file_names:\n","        file_path = os.path.join(folder_path, file_name)\n","        image = cv2.imread(file_path)\n","        image = cv2.resize(image, (32, 32))  # 调整图像大小为 32x32\n","        image = tf.cast(image, tf.float32) / 255.0  # 转换为 float32 并标准化到 [0, 1] 范围内\n","        # # 应用 Z-分数标准化（减去均值并除以标准差）\n","        # mean = tf.math.reduce_mean(image)\n","        # std = tf.math.reduce_std(image)\n","        # image = (image - mean) / std\n","\n","        # 将NaN替换为0\n","        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n","        images[file_path[14:-4]] = image\n","    def map_seq(input_str):\n","        # print(input_str)\n","        # # 查看图像的形状以调试\n","        char_images = []  # 创建一个空列表\n","        prev_index = None\n","        # 遍历输入字符串中的每个字符\n","        for index, char in enumerate(input_str):\n","            # 如果当前字符是 \"x\"，记住它的索引并退出循环\n","            if char == 'x':\n","                prev_index = index-1\n","                break\n","        # 如果没有找到 \"x\"，则记住最后一个字符的索引\n","        if prev_index is None and len(input_str) > 0:\n","            prev_index = len(input_str) - 1\n","\n","        # 遍历输入字符串中的每个字符\n","        for n in range(len(input_str)):\n","            # for char in input_str[n:n+1]:\n","            if n == prev_index:\n","                char = input_str[n]\n","                image_key = char + '_C'\n","                #print(image_key)\n","                char_tensor = tf.convert_to_tensor(images.get(image_key))\n","                char_images.append(char_tensor)\n","            elif n == 0:\n","                char = input_str[n]\n","                image_key = char + '_N'\n","                char_tensor = tf.convert_to_tensor(images.get(image_key))\n","                char_images.append(char_tensor)\n","            # 检查字符是否在images字典中\n","            elif n != prev_index:\n","                char = input_str[n]\n","                #print(char)\n","                # 如果在images字典中，将对应的图像转换为Tensor并添加到列表中\n","                char_tensor = tf.convert_to_tensor(images.get(char))\n","                char_images.append(char_tensor)\n","        char_images = np.array(char_images)\n","\n","        seq_frames = tf.stack(char_images, axis=0)\n","        return seq_frames\n","\n","    input_seq = sequence.numpy().decode(\"utf-8\")\n","    #print(\"input sequence\"+input_seq)\n","    processed_data = []\n","    # for seq in input_seq:\n","    #     print(seq)\n","    seq_frames = map_seq(input_seq)\n","    processed_data.append(seq_frames)\n","    processed_data = tf.convert_to_tensor(processed_data)\n","    #processed_data = tf.squeeze(processed_data, axis=1)\n","    return processed_data\n","\n","def preprocess_seq(filename, max_length):\n","    data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n","    sequences = data['sequence'].tolist()\n","    labels = data['label'].tolist()\n","    # print(len(sequences))\n","    # print(len(labels))\n","    processed_data = []\n","\n","    for seq, label in zip(sequences, labels):\n","        # 移除行尾空格并填充到指定长度\n","        seq = seq.strip().ljust(max_length, 'x')\n","        processed_data.append((seq, label))  # 将数据和标签打包成一个元组并添加到列表中\n","    #print(processed_data)\n","    return processed_data\n","\n","def get_max_length(filename1,filename2,max_length):\n","\n","    def count_max_length(data):\n","        sequences = data['sequence'].tolist()\n","        labels = data['label'].tolist()\n","        max_length = 0\n","        positive_sequences = []\n","        negative_sequences = []\n","        for seq, label in zip(sequences, labels):\n","            if label == 1:\n","                positive_sequences.append(seq)\n","            else:\n","                negative_sequences.append(seq)\n","            max_length = max(max_length, len(seq))\n","        return max_length\n","    # 从文件中读取每一行并将其与相应的照片相关联\n","    data1 = pd.read_excel(filename1, engine='openpyxl', keep_default_na=False, na_values=[''])  # 指定engine为'openpyxl'或'xlrd'\n","    data2 = pd.read_excel(filename2, engine='openpyxl', keep_default_na=False, na_values=[''])\n","    max_length1 = count_max_length(data1)\n","    max_length2 = count_max_length(data2)\n","    if max_length1>max_length:\n","        max_length =max_length1\n","    if max_length2>max_length1:\n","        max_length=max_length2\n","    print(\"数据集中字符最大长度:\", max_length)\n","    return max_length\n","def load_and_preprocess_data(sequences, labels, batch_size=16):\n","    # 将 sequences 转换为张量\n","    sequences = tf.constant(sequences, dtype=tf.string)\n","    labels = tf.constant(labels, dtype=tf.int32)\n","    # 创建 tf.data.Dataset，并使用 map 应用处理函数\n","    sequence_dataset = tf.data.Dataset.from_tensor_slices(sequences)\n","    labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n","    # 合并数据集\n","    dataset = tf.data.Dataset.zip((sequence_dataset, labels_dataset))\n","\n","    def map_fn(sequence, label):\n","        # 处理 sequence\n","        processed_sequence = tf.py_function(preprocess_sequence, [sequence], tf.float32)\n","        return processed_sequence,sequence, label\n","\n","    dataset = dataset.map(lambda sequence, label: map_fn(sequence, label), num_parallel_calls=tf.data.AUTOTUNE)\n","    # dataset = dataset.shuffle(buffer_size=len(sequences)).cache()\n","    # dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    return dataset\n","\n","def read_sequences(filename):\n","    \"\"\"\n","    自动识别文件格式并读取其中的序列\n","    Args:\n","      filename: 文件名\n","\n","    Returns:\n","      序列列表\n","    \"\"\"\n","    _, ext = os.path.splitext(filename)\n","    if ext == \".fasta\":\n","        with open(filename, \"r\") as f:\n","            sequences = list(SeqIO.parse(f, \"fasta\"))\n","        return [str(record.seq) for record in sequences]\n","    elif ext in [\".txt\"]:\n","        with open(filename, \"r\") as f:\n","            sequences = f.readlines()\n","        return [seq.strip() for seq in sequences]\n","    elif ext == \".xlsx\":\n","        data = pd.read_excel(filename, engine='openpyxl', keep_default_na=False, na_values=[''])\n","        return data['Sequence'].tolist()\n","    elif ext in [\".csv\"]:\n","        data = pd.read_csv(filename, na_filter=False)\n","        return data['Sequence'].tolist()\n","    else:\n","        raise ValueError(\"Unsupported file format: {}\".format(ext))\n"],"metadata":{"id":"D0sBhB77DjkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 0\n","max_length = get_max_length(train_file, val_file, max_length)\n","print(max_length)\n","\n","model = tf.keras.models.load_model(model_path)\n","model.summary()\n","\n","sequences = read_sequences(filename)\n","preprocess_predictions = preprocess_seq(sequences, max_length)\n","\n","batch_frame = np.vstack([preprocess_sequence(seq) for seq in preprocess_predictions])\n","batch_predictions = model.predict(batch_frame)\n","\n","batch_predictions_binary = (batch_predictions > 0.5).astype(\"int32\")\n","output_file_path = 'predictions.csv'\n","\n","with open(output_file_path, 'a', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    if csvfile.tell() == 0:\n","        writer.writerow([\"Sequence\", \"Label\", \"Prediction\"])\n","    for seq, predictions in zip(sequences, batch_predictions):\n","        seq_str = f'{seq}'\n","        if len(seq) > max_length:\n","            label = 'out of max length'\n","            predictions_str = 'out of max length'\n","            writer.writerow([seq_str, label, predictions_str])\n","        else:\n","            label = 0 if predictions < 0.5 else 1\n","            prediction_value = predictions[0]\n","            predictions_str = f\"{prediction_value:.6f}\"\n","            writer.writerow([seq_str, label, predictions_str])"],"metadata":{"id":"eRLoJgCsDta-"},"execution_count":null,"outputs":[]}]}